import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.tools import Tool
from langchain.utilities import SerpAPIWrapper
from langchain.agents import initialize_agent
from langchain.agents.agent_types import AgentType
from langchain.llms import HuggingFacePipeline
from langchain.schema import OutputParserException
import dotenv
import sounddevice as sd
from scipy.io.wavfile import write
import whisper
import torch
import ssl
import threading
import re

def process_audio():
    while True:
        # 1. å½•éŸ³
        print(f"ğŸ¤ å¼€å§‹å½•éŸ³ï¼ˆ{duration} ç§’ï¼‰...")
        recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')
        sd.wait()
        write(audio_filename, samplerate, recording)
        print(f"âœ… å½•éŸ³å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜ä¸ºï¼š{audio_filename}")

        # 2. Whisper æœ¬åœ°è¯†åˆ«
        print("ğŸ§  æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹å¹¶è¯†åˆ«è¯­éŸ³...")
        result = whisper_model.transcribe(audio_filename, language="en")
        recognized_text = result["text"]
        print("ğŸ§ è¯†åˆ«ç»“æœï¼š", recognized_text)

        # æ„é€  Prompt
        system_prompt = "<|im_start|>system\n you are an AI assistant.<|im_end|>"
        user_prompt = f"<|im_start|>user\n{recognized_text}<|im_end|>\n<|im_start|>assistant\n"
        prompt = system_prompt + "\n" + user_prompt
        # query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š\n> ")
        query = prompt
        pattern = r'\b(?:' + '|'.join(re.escape(word) for word in words) + r')\b'
        matches = re.findall(pattern, query, re.IGNORECASE)
        if matches:
            print("ğŸ‘‹ å†è§ï¼")
            break
        print("\nğŸ¤– å¤„ç†æŸ¥è¯¢ï¼š\n" + query)
        try:
            response = agent.invoke({"input": query, "chat_history": query})
            print("\nğŸ¤– å›ç­”ï¼š\n" + str(response))
        except OutputParserException as e:
            print("\nâŒ è§£æé”™è¯¯ï¼š", str(e))
            print("âš ï¸ è¯·æ£€æŸ¥ Prompt æ ¼å¼æˆ–ä»£ç†é…ç½®ã€‚")


# å¿½ç•¥ SSL è¯ä¹¦éªŒè¯é”™è¯¯
ssl._create_default_https_context = ssl._create_unverified_context

# å‚æ•°è®¾ç½®
words = ["bye", "See you", "Goodbye"]
duration = 10  # å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
samplerate = 16000  # Whisper æ¨èé‡‡æ ·ç‡
audio_filename = "recorded.wav"

whisper_model = whisper.load_model("large-v3") # æˆ– "base", "small", "medium"

# âœ… åŠ è½½ .env æ–‡ä»¶ä¸­çš„ API KEY
dotenv.load_dotenv()
assert os.getenv("SERPAPI_API_KEY"), "è¯·å…ˆè®¾ç½® SERPAPI_API_KEY ç¯å¢ƒå˜é‡"

# âœ… Step 1: åŠ è½½ Qwen3-4B æ¨¡å‹
print("æ­£åœ¨åŠ è½½ Qwen3-4B æ¨¡å‹ï¼Œè¯·ç¨å€™...")
model_id = "Qwen/Qwen3-4B"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, device_map="auto")

# âœ… Step 2: åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
llm = HuggingFacePipeline(pipeline=pipe)

# âœ… Step 3: è®¾ç½®è”ç½‘æœç´¢å·¥å…·ï¼ˆä½¿ç”¨ SerpAPIï¼‰
search = SerpAPIWrapper()  # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ key
search_tool = Tool(
    name="web-search",
    func=search.run,
    description="é€‚ç”¨äºéœ€è¦è”ç½‘è·å–å®æ—¶ä¿¡æ¯çš„é—®é¢˜ï¼Œæ¯”å¦‚æ–°é—»ã€å¤©æ°”ã€è‚¡å¸‚ç­‰"
)

# âœ… Step 4: åˆå§‹åŒ–æ™ºèƒ½ä»£ç†
agent = initialize_agent(
    tools=[search_tool],
    llm=llm,
    # agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=False
)

# âœ… Step 5: ç”¨æˆ·è¾“å…¥å¹¶æ‰§è¡ŒæŸ¥è¯¢
if __name__ == "__main__":
    print("\næ¬¢è¿ä½¿ç”¨ Qwen3-4B + å®æ—¶è”ç½‘æœç´¢ä»£ç†")
    # å¯åŠ¨å½•éŸ³å’Œå¤„ç†çº¿ç¨‹
    thread = threading.Thread(target=process_audio)
    thread.start()
